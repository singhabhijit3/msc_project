baseline_1_results: num_steps=20, batch_size=20, hidden_size=200, 1 LSTM, no dropout, ADAM, (E9-5.128)
zaremba_results: num_steps=20, batch_size=20, hidden_size=200, 2 LSTM, no dropout, ADAM, (E8-5.159)

baseline_1_results_2: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E6-4.983)
zaremba_results_2: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, no dropout, ADAM, (E6-5.065)

zaremba_results_3: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 3 dropout layers (0.5), ADAM, (E15-4.951)

From here it uses early stopping:

No. of dropout layers:

zaremba_results_4: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 3 dropout layers (0.5), ADAM, (E17-4.954)

zaremba_results_5: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E11-4.899)

zaremba_results_6: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 1 dropout layer (0.5), ADAM, (E8-4.968)

Learning Rates:

baseline_1_lr_00001: Did not early stop for 20 epochs, best val loss 5.171
baseline_1_lr_001: E3-5.317
baseline_1_lr_01: extremely bad
baseline_1_lr_1: extremely bad
baseline_1_lr_10: extremely bad
baseline_1_lr_100: extremely bad

zaremba_lr_00001: Did not early stop for 20 epochs, best val loss 5.276
zaremba_lr_001: Did not early stop for 20 epochs, best val loss 5.362
zaremba_lr_01: extremely bad
zaremba_lr_1: extremely bad
zaremba_lr_10: extremely bad
zaremba_lr_100: extremely bad

baseline_1_results_3: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.933)
zaremba_results_7: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E12-4.955)

baseline_1_results_4: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.934)
zaremba_results_8: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E11-4.960)

baseline_1_results_5: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.930)
zaremba_results_9: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E13-4.955)

baseline_1_results_6: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.968)
zaremba_results_10: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E13-4.962)

baseline_1_results_7: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.978)
zaremba_results_11: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E10-4.977)

baseline_1_results_8: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E4-4.961)
zaremba_results_12: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E12-4.960)

baseline_1_results_9: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.950)
zaremba_results_13: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E9-5.020)

Learning Rate Schedules:

baseline_1_lrs_rms: E4-5.171
baseline_1_lrs_adam: E5-4.926 (patience=3, factor=0.5)
baseline_1_lrs_adam2: E5-4.962 (patience=2, factor=0.5)
baseline_1_lrs_adam3: E5-4.943 (patience=2, factor=0.1)
baseline_1_lrs_adam4: E5-4.937 (patience=1, factor=0.1)
baseline_1_lrs_adam5: E9-5.289 (patience=1, factor=0.1, lr=0.01)

zaremba_lrs_rms: E11-5.133
zaremba_lrs_adam: E18-4.951 (patience=3, factor=0.5)
zaremba_lrs_adam2: E28-4.914 (patience=2, factor=0.5)
zaremba_lrs_adam3: E30-4.860 (patience=2, factor=0.1)
zaremba_lrs_adam4: E21-4.872 (patience=1, factor=0.1)
zaremba_lrs_adam5: E21-5.264 (patience=1, factor=0.1, lr=0.01)


