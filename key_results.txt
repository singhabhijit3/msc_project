baseline_1_results: num_steps=20, batch_size=20, hidden_size=200, 1 LSTM, no dropout, ADAM, (E9-5.128)
zaremba_results: num_steps=20, batch_size=20, hidden_size=200, 2 LSTM, no dropout, ADAM, (E8-5.159)

baseline_1_results_2: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E6-4.983)
zaremba_results_2: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, no dropout, ADAM, (E6-5.065)

zaremba_results_3: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 3 dropout layers (0.5), ADAM, (E15-4.951)

From here it uses early stopping:

No. of dropout layers:

zaremba_results_4: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 3 dropout layers (0.5), ADAM, (E17-4.954)

zaremba_results_5: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E11-4.899)

zaremba_results_6: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 1 dropout layer (0.5), ADAM, (E8-4.968)

Learning Rates:

baseline_1_lr_00001: Did not early stop for 20 epochs, best val loss 5.171
baseline_1_lr_001: E3-5.317
baseline_1_lr_01: extremely bad
baseline_1_lr_1: extremely bad
baseline_1_lr_10: extremely bad
baseline_1_lr_100: extremely bad

zaremba_lr_00001: Did not early stop for 20 epochs, best val loss 5.276
zaremba_lr_001: Did not early stop for 20 epochs, best val loss 5.362
zaremba_lr_01: extremely bad
zaremba_lr_1: extremely bad
zaremba_lr_10: extremely bad
zaremba_lr_100: extremely bad

baseline_1_results_3: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.933)
zaremba_results_7: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E12-4.955)

baseline_1_results_4: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.934)
zaremba_results_8: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E11-4.960)

baseline_1_results_5: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.930)
zaremba_results_9: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E13-4.955)

baseline_1_results_6: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.968)
zaremba_results_10: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E13-4.962)

baseline_1_results_7: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.978)
zaremba_results_11: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E10-4.977)

baseline_1_results_8: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E4-4.961)
zaremba_results_12: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E12-4.960)

baseline_1_results_9: num_steps=35, batch_size=20, hidden_size=650, 1 LSTM, no dropout, ADAM, (E5-4.950)
zaremba_results_13: num_steps=35, batch_size=20, hidden_size=650, 2 LSTM, 2 dropout layers (0.5), ADAM, (E9-5.020)


